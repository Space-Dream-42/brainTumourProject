{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276bae4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Simple Segmentation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88614b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from data_loading import BraTS_Dataset\n",
    "from dataset_utils import plot_batch, crop_batch, decrop_batch, split_cube, slice_cube\n",
    "from data_loading import get_train_test_iters\n",
    "torch.manual_seed(42)\n",
    "from Architectures.unet_3d import UNet3D\n",
    "from Architectures.unet_2d import UNet2D\n",
    "from train import train_model\n",
    "from custom_losses import get_loss, DiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa390054",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Task01_BrainTumour/cropped\\imagesTr\n",
      "../Task01_BrainTumour/cropped\\labelsTr\n",
      "../Task01_BrainTumour/cropped\\imagesTs\n",
      "../Task01_BrainTumour/cropped\\labelsTs\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = get_train_test_iters('../Task01_BrainTumour/cropped', batch_size=1, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6794d3e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b87e8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = UNet3D(num_modalities=4, num_classes=4, img_height=96, img_width=96).to(device)\n",
    "#model = UNet2D().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = DiceLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'detatch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m400\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Deep_Learning_Project\\Code\\train.py:24\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, optimizer, loss, epochs, device, has_minicubes, train_iter, steps_per_epoch)\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m160\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m: \u001B[38;5;66;03m#check if model is 2D\u001B[39;00m\n\u001B[0;32m     23\u001B[0m         batch \u001B[38;5;241m=\u001B[39m slice_cube(train_iter\u001B[38;5;241m.\u001B[39mnext())\n\u001B[1;32m---> 24\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mget_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_minicubes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m save_rate \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\Deep_Learning_Project\\Code\\custom_losses.py:59\u001B[0m, in \u001B[0;36mget_loss\u001B[1;34m(model, loss_fn, has_minicubes, step, device, batch)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_loss\u001B[39m(model, loss_fn, has_minicubes, step, device, batch):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_minicubes:\n\u001B[1;32m---> 59\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mget_minicube_batch_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     61\u001B[0m         prediction_batch \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m][step\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m160\u001B[39m, :, :, :, :]\u001B[38;5;241m.\u001B[39mto(device))\n",
      "File \u001B[1;32m~\\Deep_Learning_Project\\Code\\custom_losses.py:36\u001B[0m, in \u001B[0;36mget_minicube_batch_loss\u001B[1;34m(model, loss_fn, minicube_batch, step, device)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m#if step % 4 == 0:\u001B[39;00m\n\u001B[0;32m     35\u001B[0m voxel_logits_batch \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(minicube_batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m][:number_of_cubes, :, :, :, :]\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 36\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvoxel_logits_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mminicube_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43mnumber_of_cubes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Deep_Learning_Project\\Code\\custom_losses.py:12\u001B[0m, in \u001B[0;36mDiceLoss.forward\u001B[1;34m(self, inputs, targets)\u001B[0m\n\u001B[0;32m     10\u001B[0m loss_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(inputs[\u001B[38;5;241m0\u001B[39m])):\n\u001B[1;32m---> 12\u001B[0m     current_class \u001B[38;5;241m=\u001B[39m \u001B[43mtargets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetatch\u001B[49m()\n\u001B[0;32m     13\u001B[0m     current_class[current_class \u001B[38;5;241m!=\u001B[39m i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     14\u001B[0m     current_class[current_class \u001B[38;5;241m==\u001B[39m i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'detatch'"
     ]
    }
   ],
   "source": [
    "losses = train_model(model, optim, loss, 2, device, True, train_iter, steps_per_epoch = 400)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c29e8e8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test & debug model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a922f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample a minicube batch\n",
    "minicube_batch = split_cube(train_iter.next()) # 8 minicubes per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b630e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voxel_logits_batch = model.forward(minicube_batch['image'][:1,:,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762bd25",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training loop utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5117a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86aaae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_minicube_batch_loss(minicube_batch, step, device):\n",
    "    if step%4 == 0:\n",
    "        voxel_logits_batch = model.forward(minicube_batch['image'][:2,:,:,:,:])\n",
    "        loss = criterion(voxel_logits_batch, minicube_batch['label'][:2,:,:,:].long().to(device))\n",
    "        return loss\n",
    "    \n",
    "    elif step%4 == 1:\n",
    "        voxel_logits_batch = model.forward(minicube_batch['image'][2:4,:,:,:,:])\n",
    "        loss = criterion(voxel_logits_batch, minicube_batch['label'][2:4,:,:,:].long().to(device))\n",
    "        return loss\n",
    "    \n",
    "    elif step%4 == 2:\n",
    "        voxel_logits_batch = model.forward(minicube_batch['image'][4:6,:,:,:,:])\n",
    "        loss = criterion(voxel_logits_batch, minicube_batch['label'][4:6,:,:,:].long().to(device))\n",
    "        return loss\n",
    "    \n",
    "    else:\n",
    "        voxel_logits_batch = model.forward(minicube_batch['image'][6:,:,:,:,:])\n",
    "        loss = criterion(voxel_logits_batch, minicube_batch['label'][6:,:,:,:].long().to(device))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca806af8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735f80e",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "\n",
    "\n",
    "# training settings\n",
    "epochs = 4\n",
    "steps_per_epoch = 400\n",
    "losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        if step%4 == 0:\n",
    "            # Get a new minicube batch\n",
    "            minicube_batch = split_cube(train_iter.next())\n",
    "        \n",
    "        loss = get_minicube_batch_loss(minicube_batch, step, device=device)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if step%20 == 0:\n",
    "            print(f'epoch {epoch}: step {step:3d}: loss={loss:3.3f}')\n",
    "            path = f'../Weights/weights_epoch{epoch}_step{step}_loss{loss:3.3f}.h5'\n",
    "            torch.save(model.state_dict(), path)\n",
    "        \n",
    "        # backprop loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebb103",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "losses_array = [l.detach().cpu().numpy() for l in losses]\n",
    "plt.plot(losses_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff26c89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load weights\n",
    "weights_filename = 'weights_epoch3_step300_loss0.026.h5'\n",
    "inference_model = SmallBTSegNet(num_modalities=4, num_classes=4, img_height=96, img_width=96).to(device)\n",
    "inference_model.load_state_dict(torch.load('../Weights/' + weights_filename))\n",
    "_ = inference_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169b86a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89bf9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# predict batch\n",
    "voxel_logits_batch = inference_model.forward(minicube_batch['image'][None,4,:,:,:,:])\n",
    "\n",
    "# get loss\n",
    "loss = criterion(voxel_logits_batch, minicube_batch['label'][None,4,:,:,:].long().to(device))\n",
    "print(f'loss: {loss.item():3.3f}')\n",
    "\n",
    "sm = nn.Softmax(dim=1)\n",
    "voxel_probs_batch = sm(voxel_logits_batch)\n",
    "print(voxel_probs_batch.shape)\n",
    "\n",
    "probs, out = torch.max(voxel_probs_batch, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d64fb1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('prediction:')\n",
    "plt.imshow(out[0, 5, :,:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac150f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('label:')\n",
    "plt.imshow(minicube_batch['label'][4, 5, :, :].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}