{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e82eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.vision import StandardTransform\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import json\n",
    "import os\n",
    "\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662bb57",
   "metadata": {},
   "source": [
    "# File structure:\n",
    "```\n",
    "BT_Segmentation_Project\n",
    "    |_ Code\n",
    "       |_ Data_Extraction.ipynb     (this file)\n",
    "       |_ ...\n",
    "    |_ Task01_BrainTumour           (Dataset)\n",
    "       |_ dataset.json\n",
    "       |_ imagesTr                  (here are the compressed nii-files)\n",
    "       |_ labelsTr\n",
    "       |_ imagesTs\n",
    "       |_ extracted                 (here are the numpy files)\n",
    "           |_ imagesTr\n",
    "           |_ labelsTr\n",
    "           |_ imagesTs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63e8e2",
   "metadata": {},
   "source": [
    "# Extracting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c703557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir:    /media/z/Ubuntu-Storage/BT_Segmentation_Project/Code\n",
      "parent_dir:  /media/z/Ubuntu-Storage/BT_Segmentation_Project\n",
      "dataset_dir: /media/z/Ubuntu-Storage/BT_Segmentation_Project/Task01_BrainTumour\n",
      "data not extracted yet.\n",
      " -> creating new extracted dir.\n"
     ]
    }
   ],
   "source": [
    "# prepare extracting files:\n",
    "# make new dirs etc\n",
    "\n",
    "root_dir = os. getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(root_dir, os.pardir))\n",
    "dataset_dir = os.path.join(parent_dir, 'Task01_BrainTumour')\n",
    "print(f'root_dir:    {root_dir}')\n",
    "print(f'parent_dir:  {parent_dir}')\n",
    "print(f'dataset_dir: {dataset_dir}')\n",
    "\n",
    "# imagesTr: training images\n",
    "# labelsTr: training labels\n",
    "# imagesTs: test images\n",
    "\n",
    "extracted_dir = os.path.join(dataset_dir, 'extracted')\n",
    "imgTr_dir = os.path.join(extracted_dir, 'imagesTr')\n",
    "labelsTr_dir = os.path.join(extracted_dir, 'labelsTr')\n",
    "imgTs_dir = os.path.join(extracted_dir, 'imagesTs')\n",
    "\n",
    "if not os.path.exists(dataset_dir + '/extracted'):\n",
    "    extracted = False\n",
    "    print('data not extracted yet.\\n -> creating new extracted dir.')\n",
    "    for dir_path in [extracted_dir, imgTr_dir, labelsTr_dir, imgTs_dir]:\n",
    "        os.mkdir(dir_path)\n",
    "else:\n",
    "    extracted = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689dceef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'description', 'reference', 'licence', 'release', 'tensorImageSize', 'modality', 'labels', 'numTraining', 'numTest', 'training', 'test'])\n",
      "\n",
      "Scans: {'0': 'FLAIR', '1': 'T1w', '2': 't1gd', '3': 'T2w'}\n",
      "\n",
      "Labels: {'0': 'background', '1': 'edema', '2': 'non-enhancing tumor', '3': 'enhancing tumour'}\n",
      "\n",
      "#Training: 484\n",
      "#Test: 266\n",
      "\n",
      "Training-files paths: {'image': './imagesTr/BRATS_457.nii.gz', 'label': './labelsTr/BRATS_457.nii.gz'}\n",
      "Test-files path: ./imagesTs/BRATS_557.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# load the json dataset-info-file as a dictionary\n",
    "with open(dataset_dir + '/dataset.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data.keys())\n",
    "    print('\\nScans:', data['modality'])\n",
    "    print('\\nLabels:', data['labels'])\n",
    "    print('\\n#Training:', data['numTraining'])\n",
    "    print('#Test:', data['numTest'])\n",
    "    \n",
    "    train_filenames = data['training']\n",
    "    test_filenames = data['test']\n",
    "    \n",
    "print('\\nTraining-files paths:', train_filenames[0])\n",
    "print('Test-files path:', test_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49451ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished extracting training files.\n",
      "finished extracting test files.\n"
     ]
    }
   ],
   "source": [
    "# extract files\n",
    "# needs about 110-120 GB disk-storage\n",
    "# or modify the code (break after e.g. 10 iterations)\n",
    "# to only unpack a couple of samples \n",
    "\n",
    "if extracted:\n",
    "    print('files are already extracted.')\n",
    "\n",
    "else:\n",
    "    num_of_train_files = len(train_filenames)\n",
    "    num_of_test_files = len(test_filenames)\n",
    "    \n",
    "    for i, img_path_dict in enumerate(train_filenames):\n",
    "        print(f'extracting training_file {i}/{num_of_train_files}', end=\"\\r\")\n",
    "        image_path_gz = img_path_dict['image'][2:]\n",
    "        label_path_gz = img_path_dict['label'][2:]\n",
    "        \n",
    "        # extract and save image\n",
    "        img_path = os.path.join(dataset_dir, image_path_gz) # nii-image path\n",
    "        sitk_img = sitk.ReadImage(img_path) # read the nii-image\n",
    "        img = sitk.GetArrayFromImage(sitk_img) # img to numpy array\n",
    "        extracted_img_path = os.path.join(imgTr_dir, str(i))\n",
    "        np.save(extracted_img_path, img)\n",
    "        \n",
    "        # extract and save label\n",
    "        label_path = os.path.join(dataset_dir, label_path_gz) # nii-image path\n",
    "        sitk_label_img = sitk.ReadImage(label_path) # read the nii-image\n",
    "        img_label = sitk.GetArrayFromImage(sitk_label_img) # img to numpy array\n",
    "        extracted_label_path = os.path.join(labelsTr_dir, str(i))\n",
    "        np.save(extracted_label_path, img_label)\n",
    "    print('finished extracting training files.')\n",
    "    \n",
    "    for i, img_path in enumerate(test_filenames):\n",
    "        print(f'extracting testing_file {i}/{num_of_test_files}', end=\"\\r\")\n",
    "        image_path_gz = img_path[2:]\n",
    "        \n",
    "        # extract and save test image\n",
    "        img_path = os.path.join(dataset_dir, image_path_gz) # nii-image path\n",
    "        sitk_img = sitk.ReadImage(img_path) # read the nii-image\n",
    "        img = sitk.GetArrayFromImage(sitk_img) # img to numpy array\n",
    "        extracted_img_path = os.path.join(imgTs_dir, str(i))\n",
    "        np.save(extracted_img_path, img)\n",
    "    print('finished extracting test files.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
