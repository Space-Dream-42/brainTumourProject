{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f80684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.vision import StandardTransform\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import json\n",
    "import os\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "#from Dataset_Utils import BraTS_TrainingDataset\n",
    "#from Dataset_Utils import BraTS_TestDataset\n",
    "#from Dataset_Utils import plot_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecde9f",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488ae7a",
   "metadata": {},
   "source": [
    "<img src=\"Lecture_Architecture.png\" alt=\"Lecture_Architecture\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4d0986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input img:\n",
      "[[0 0 1 0 0]\n",
      " [1 1 0 0 1]\n",
      " [0 0 1 1 0]\n",
      " [1 0 1 1 0]\n",
      " [1 0 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0413,  0.0571, -0.0997,  0.0198, -0.1223],\n",
       "         [ 0.1456,  0.1739,  0.2964,  0.2903, -0.1197],\n",
       "         [ 0.1347,  0.2592,  0.2673,  0.2048, -0.0438],\n",
       "         [ 0.1642,  0.1689,  0.2810,  0.2325,  0.0128],\n",
       "         [-0.0700,  0.0719,  0.0996,  0.1264,  0.0419]],\n",
       "\n",
       "        [[ 0.1570,  0.2588,  0.1248,  0.1236,  0.0873],\n",
       "         [ 0.0662, -0.1238, -0.0896,  0.0719, -0.0793],\n",
       "         [ 0.1347, -0.1356,  0.1285, -0.0952, -0.2496],\n",
       "         [ 0.1424, -0.1869,  0.1257, -0.1513, -0.1662],\n",
       "         [-0.0091, -0.2744, -0.1468, -0.2276, -0.0937]],\n",
       "\n",
       "        [[-0.0357,  0.1466,  0.1671,  0.0828,  0.2482],\n",
       "         [ 0.0511,  0.1596,  0.0040,  0.2370,  0.3132],\n",
       "         [ 0.0318,  0.0936,  0.3153,  0.1655,  0.0958],\n",
       "         [-0.0062,  0.2156,  0.1487,  0.1155,  0.1994],\n",
       "         [ 0.1673,  0.2384,  0.0726,  0.1179,  0.1024]],\n",
       "\n",
       "        [[ 0.1378,  0.2131,  0.3136,  0.1677,  0.1810],\n",
       "         [-0.0130,  0.0499, -0.1145, -0.0363,  0.2648],\n",
       "         [-0.0543, -0.0592, -0.0282,  0.0515,  0.1517],\n",
       "         [-0.0399,  0.0604, -0.0445, -0.0548,  0.0706],\n",
       "         [-0.0107, -0.0963, -0.1049, -0.1716,  0.0052]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmallSegNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=4, num_classes=4, img_height=5, img_width=5):\n",
    "        super(SmallSegNet, self).__init__()\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        self.layers = nn.Sequential(\n",
    "        \n",
    "        # decrease x,y and increase channels\n",
    "            # Conv\n",
    "            nn.Conv2d(in_channels,4,2,stride=1,padding=0),\n",
    "            # nn.Batchnorm\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        # change number of channels\n",
    "            # 1x1 Conv\n",
    "            nn.Conv2d(4,num_classes*2,1,stride=1,padding=0),\n",
    "            # nn.Batchnorm\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        # increase x,y and decrease channels\n",
    "            # Transpose-Conv\n",
    "            nn.ConvTranspose2d(num_classes*2, num_classes, 2, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x)\n",
    "        x = self.layers(x)\n",
    "        # x = self.sm(x)\n",
    "        # Skip Softmax because Torch CrossEntropyLoss\n",
    "        # takes logits and already applies Softmax.\n",
    "        return x\n",
    "\n",
    "# summary\n",
    "# print(SmallSegNet())\n",
    "\n",
    "model = SmallSegNet(in_channels=1, num_classes=4, img_height=5, img_width=5)\n",
    "\n",
    "x = np.random.randint(2, size=(5,5))\n",
    "print('input img:')\n",
    "print(x)\n",
    "\n",
    "y_hat = model.forward(x[None,:])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272cc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1570, 0.2588, 0.3136, 0.1677, 0.2482],\n",
      "        [0.1456, 0.1739, 0.2964, 0.2903, 0.3132],\n",
      "        [0.1347, 0.2592, 0.3153, 0.2048, 0.1517],\n",
      "        [0.1642, 0.2156, 0.2810, 0.2325, 0.1994],\n",
      "        [0.1673, 0.2384, 0.0996, 0.1264, 0.1024]], grad_fn=<MaxBackward0>)\n",
      "tensor([[1, 1, 3, 3, 2],\n",
      "        [0, 0, 0, 0, 2],\n",
      "        [1, 0, 2, 0, 3],\n",
      "        [0, 2, 0, 0, 2],\n",
      "        [2, 2, 0, 0, 2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2646c973a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs, out = torch.max(y_hat, dim=0)\n",
    "print(probs)\n",
    "print(out)\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36651fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# make dataset\n",
    "# train\n",
    "\n",
    "# save params\n",
    "# torch.save(model.state_dict(), 'SmallSegNet.pt')\n",
    "\n",
    "# load params\n",
    "# model = SmallSegNet()\n",
    "# model.load_state_dict(torch.load('SmallSegNet.pt'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbd69",
   "metadata": {},
   "source": [
    "## Crossentropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e43e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label =  torch.tensor(\n",
    "       [[3, 1, 1, 1, 3],\n",
    "        [3, 3, 0, 3, 3],\n",
    "        [0, 3, 3, 3, 3],\n",
    "        [1, 3, 0, 3, 3],\n",
    "        [0, 3, 3, 3, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac8b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossentropy Loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ed9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 5])\n",
      "torch.Size([1, 4, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(y_hat.shape)\n",
    "y_hat_batch = y_hat[None,:]\n",
    "print(y_hat_batch.shape)\n",
    "\n",
    "print(label.shape)\n",
    "label_batch = label[None,:]\n",
    "print(label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0359fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3896, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(y_hat_batch, label_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d923d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
