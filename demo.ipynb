{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276bae4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "BraTS Challenge Demo-Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0612b4e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from dataset_utils import slice_cube, split_cube\n",
    "from visualization_utils import plot_confusion_matrix, plot_loss, animate_cube, get_positives_negatives_from_cm\n",
    "from data_loading import get_train_test_iters\n",
    "torch.manual_seed(42)\n",
    "from Architectures.unet_3d import UNet3D\n",
    "from Architectures.unet_3d_context import UNet3D_Mini\n",
    "from Architectures.unet_2d import UNet2D\n",
    "from train import train_model\n",
    "from custom_losses import DiceLoss, FocalTverskyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4e7dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657dcef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207b0554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083597f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830d29a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "train_3d = True\n",
    "add_context = False\n",
    "compute_test_loss = True\n",
    "epochs = 60\n",
    "learning_rate = 0.1\n",
    "\n",
    "# init model\n",
    "if train_3d:\n",
    "    if add_context:\n",
    "        model = UNet3D_Mini(num_modalities=4, num_classes=4).to(device)\n",
    "    else:\n",
    "        model = UNet3D(num_modalities=4, num_classes=4).to(device)\n",
    "else:\n",
    "    model = UNet2D().to(device)\n",
    "weights_path = os.path.join('..','Weights')\n",
    "weights_filename = 'UNet3D_epoch14_loss0.073.h5'\n",
    "#model.load_state_dict(torch.load(os.path.join(weights_path, weights_filename)))\n",
    "# init optimizer and loss_fn\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900cbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallSegNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=4, num_classes=4, img_height=5, img_width=5):\n",
    "        super(SmallSegNet, self).__init__()\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.layers = nn.Sequential(\n",
    "        \n",
    "        # decrease x,y and increase channels\n",
    "            # Conv\n",
    "            nn.Conv3d(in_channels,8,2,stride=1,padding=0),\n",
    "            # nn.Batchnorm\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        # change number of channels\n",
    "            # 1x1 Conv\n",
    "            nn.Conv3d(8,8,1,stride=1,padding=0),\n",
    "            # nn.Batchnorm\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        # increase x,y and decrease channels\n",
    "            # Transpose-Conv\n",
    "            nn.ConvTranspose3d(8, num_classes, 2, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # x = self.sm(x)\n",
    "        # Skip Softmax because Torch CrossEntropyLoss\n",
    "        # takes logits and already applies Softmax.\n",
    "        return x\n",
    "\n",
    "model = SmallSegNet(in_channels=4, num_classes=4, img_height=96, img_width=96).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84635074",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataset_path = os.path.join('..', 'Task01_BrainTumour', 'cropped_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069db7dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SmallSegNet:\n",
      "epoch 0: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 1: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 2: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 3: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 4: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 5: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 6: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 7: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 8: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 9: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 10: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 11: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 12: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 13: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 14: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 15: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 16: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 17: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 18: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 19: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 20: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 21: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 22: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 23: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 24: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 25: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 26: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 27: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 28: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 29: epoch_train_loss=3.419, epoch_test_loss=2.858\n",
      "epoch 30: epoch_train_loss=3.419, epoch_test_loss=2.858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_test_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Deep_Learning_Project\\Code\\train.py:57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, loss_fn, epochs, device, dataset_path, batch_size, train_3d, add_context, compute_test_loss)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_test_loss:\n\u001b[0;32m     55\u001b[0m     epoch_test_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, raw_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_iter):\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# 3D model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m train_3d:\n\u001b[0;32m     60\u001b[0m             batch \u001b[38;5;241m=\u001b[39m split_cube(raw_batch, add_context)  \u001b[38;5;66;03m# Get a new batch of 3D minicubes\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Deep_Learning_Project\\Code\\data_loading.py:51\u001b[0m, in \u001b[0;36mBraTS_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     49\u001b[0m img_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimagesTr[idx]\n\u001b[0;32m     50\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgTr_dir, img_filename)\n\u001b[1;32m---> 51\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# load label\u001b[39;00m\n\u001b[0;32m     54\u001b[0m label_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabelsTr[idx]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\numpy\\lib\\npyio.py:430\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_Project\\lib\\site-packages\\numpy\\lib\\format.py:756\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    768\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(f'training {model.__class__.__name__}:')\n",
    "train_losses, test_losses = train_model(model, optim, loss_fn, epochs, device, dataset_path, batch_size, train_3d, add_context, compute_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d70c15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f9402",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Loss curve using {loss_fn}:')\n",
    "plot_loss(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c487f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc2836",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights_path = os.path.join('..','Weights')\n",
    "weights_filename = 'SmallSegNet_epoch26_loss149.591.h5'\n",
    "if train_3d:\n",
    "    if add_context:\n",
    "        inference_model = UNet3D_Mini(num_modalities=4, num_classes=4).to(device)\n",
    "    else:\n",
    "        inference_model = UNet3D(num_modalities=4, num_classes=4).to(device)\n",
    "else:\n",
    "    inference_model = UNet2D().to(device)\n",
    "inference_model = SmallSegNet(in_channels=4, num_classes=4, img_height=96, img_width=96).to(device)\n",
    "inference_model.load_state_dict(torch.load(os.path.join(weights_path, weights_filename)))\n",
    "inference_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f2302",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e738716",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = get_train_test_iters(dataset_path, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "batch = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cddc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "minicube_batch = split_cube(batch, add_context) # split cubes into minicubes\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "for minicube_idx in range(1):\n",
    "    x = minicube_batch['image'][None,minicube_idx,:,:,:,:].to(device)\n",
    "    voxel_logits = model.forward(x).cpu()\n",
    "    voxel_probs = sm(voxel_logits)\n",
    "    voxel_probs[0, 0] -= 0.9\n",
    "    \n",
    "    probs, out = torch.max(voxel_probs, dim=1)\n",
    "plt.imshow(out[0, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d85ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ani = animate_cube(inference_model, batch, add_context, device, train_3d)\n",
    "plt.show()\n",
    "#del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=1)\n",
    "temp = split_cube(batch, False)['image'][None,0,:,:,:,:].to(device)\n",
    "print(temp.shape)\n",
    "voxel_logits = model.forward(temp).cpu()\n",
    "voxel_probs = sm(voxel_logits)\n",
    "probs, out = torch.max(voxel_probs, dim=1)\n",
    "\n",
    "print(probs[0, 79, 70:75, 70:75].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1218b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = torch.tensor([[[100, -100, -100, -100],[-100, 100, -100, -100], [-100, -100, 100, -100],[-100, -100, -100,100]]]).float()\n",
    "print(my_input.shape)\n",
    "my_label = torch.tensor([[[0, 1, 2, 3]]])\n",
    "print(my_label.shape)\n",
    "print(loss_fn(my_input, my_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_class = my_label.clone()\n",
    "current_class[current_class != 3] = 10\n",
    "current_class[current_class == 3] = 11\n",
    "current_class = current_class-10\n",
    "print(current_class)\n",
    "print(dice_loss_one_image(torch.tensor([0, 0, 0, 100]), torch.tensor([[[0, 0, 0, 1]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ac38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sigmoid(torch.tensor([0, 0, 0, 100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e9484",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19997f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73302aaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train_iter, test_iter = get_train_test_iters(os.path.join('..', 'Task01_BrainTumour', 'cropped'), batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "#cf_matrix_visu, cf_matrix = plot_confusion_matrix(test_iter, model, train_3d, add_context, device=device)\n",
    "#cf_matrix_visu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abb973",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#get_positives_negatives_from_cm(cf_matrix.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
